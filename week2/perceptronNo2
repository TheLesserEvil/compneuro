Author: Athena.
Please don't just copy; use it as a guide. 
If you think there is any problem, it would be nice to mention it (it is probable that I made some mistakes)


clear all;

DATA='random';
%DATA='mnist';	% classes 3 and 7

switch DATA
case 'random',
	p=50;			% number of patterns
	n=50;			% number of dimensions
	xi=randi(2,p,n+1)-1;    % inputs xi(:,1:n)=0,1 xi(:,n+1)=-1;
	xi(:,n+1)=-1;
	z=2*randi(2,p,1)-3;		% output z(:,1)=-1,1
	
	x=xi.*(z*ones(1,n+1));
case 'mnist',
	 load mnistAll.mat
    Xmnist=mnist.train_images;
    ymnist=mnist.train_labels;

    X3=Xmnist(:,:,find(ymnist==3)); % images of 3's
    X7=Xmnist(:,:,find(ymnist==7)); % images of 7's
	
    n=size(X3,1).^2;
	x3=-reshape(X3,n,size(X3,3));	% input data of 3's multiplied by class label -1
	x7=reshape(X7,n,size(X7,3));		% input data of 7's multiplied by class label 
	x3(n+1,:)=-1;
	x7(n+1,:)=1;
	x=[x3,x7]';
	x=double(x);	% matlab does not like the mnist data format
	p=size(x,1);
	
end;

w=randn(1,n+1);			% weights w(1:n) threshold w(n+1)
eta=1;

maxiter=1000;

% write your perceptron learning algorithm (I call a function I created)
[converged,w] = learn(x,w,p,eta,maxiter);

if ~converged
    disp('The perceptron learning rule did not converge.');
end;

%%
% exercise 3:
% Reconstruct the curve C(p,n) as a function of p for n=50 in the following way. 
% For each p construct a number of learning problems randomly and compute the 
% fraction of these problems for which the perceptron learning rule converges. 
% Plot this fraction versus p.

n = 50;             % number of dimensions
eta = 1;            % learning rate
maxiter = 1000;     % maximum iterations
num_problems = 25;  % number of problems that will be tested with each pattern
p_test = 5:5:200;   % all the p values that are tested
convergence_rates = zeros(1,length(p_test));

for p = p_test % for all p values
    sum_converged = 0;
    for problem = 1:num_problems % perform num_problems random problems
        rng shuffle % creates a different seed each time
        
        xi=randi(2,p,n+1)-1;    % inputs xi(:,1:n)=0,1 xi(:,n+1)=-1;
        xi(:,n+1)=-1;
        z=2*randi(2,p,1)-3;		% output z(:,1)=-1,1
        
        x=xi.*(z*ones(1,n+1));
        w=randn(1,n+1);
        
        [converged,w] = learn(x,w,p,eta,maxiter);
        sum_converged = sum_converged + converged; % (true=1, false=0)
    end
    
    convergence_rates(p/5) = sum_converged/num_problems;
end

figure;
plot(p_test,convergence_rates);
title('Convergence rate vs. p');
xlabel('p'); ylabel('Convergence rate');


function [ converged,w ] = learn( x,w,p,eta,maxiter )
%LEARN This function applies the learning rule in a perceptron
    % Input: 
    % x (x = xi * zeta)
    % w (weights)
    % p (number of patterns)
    % eta (learning rate - default=1)
    % maxiter (maximum number of iterations - default=1000)
    %
    % Output:
    % converged (true or false) Indicates if the perceptron learning rule converged.
    % w (float) The learned weights
    
    % HOW THE ALGORITHM WORKS 
    % we have inputs xi and outputs zeta. 
    % We also know that w * x^mu = w * xi^mu * zeta^mu
    % We need to train w so that sign(w * x^mu) = 1, for all x.
    % To train w, we adjust all w such that sign(w * x^mu) ~= 1 as follows:
    % wNew = wOld + Delta(w) = wOld + eta * x^mu 
    % note: The full formula is wNew = wOld + eta * x^mu * theta(-w * x^mu),
    % but theta is equal to 1 in the case that w * x^mu <= 0, because
    % theta(positive number) = 1.
    
    % if no eta value was given, set default eta number
    if nargin < 4
        eta = 1;
    end
    
    % if no maxiter value was given, set default max iteration number
    if nargin < 5
        maxiter = 1000;
    end

    % start learning
    for i = 1:maxiter
        converged = true; % it will remain true only if ALL patterns are ok
        for mu = 1:p      % check ALL patterns
            if sign(w * x(mu,:)') ~= 1
                w = w + eta * x(mu,:);
                converged = false;
            end
        end

        if converged
            break;
        end
    end

end


% DIFFERENT FILE 

function [ converged,w ] = learn( x,w,p,eta,maxiter )
%LEARN This function applies the learning rule in a perceptron
    % Input: 
    % x (x = xi * zeta)
    % w (weights)
    % p (number of patterns)
    % eta (learning rate - default=1)
    % maxiter (maximum number of iterations - default=1000)
    %
    % Output:
    % converged (true or false) Indicates if the perceptron learning rule converged.
    % w (float) The learned weights
    
    % HOW THE ALGORITHM WORKS 
    % we have inputs xi and outputs zeta. 
    % We also know that w * x^mu = w * xi^mu * zeta^mu
    % We need to train w so that sign(w * x^mu) = 1, for all x.
    % To train w, we adjust all w such that sign(w * x^mu) ~= 1 as follows:
    % wNew = wOld + Delta(w) = wOld + eta * x^mu 
    % note: The full formula is wNew = wOld + eta * x^mu * theta(-w * x^mu),
    % but theta is equal to 1 in the case that w * x^mu <= 0, because
    % theta(positive number) = 1.
    
    % if no eta value was given, set default eta number
    if nargin < 4
        eta = 1;
        maxiter = 1000;
    end
    
    % if no maxiter value was given, set default max iteration number
    if nargin < 5
        maxiter = 1000;
    end

    % start learning
    for i = 1:maxiter
        converged = true; % it will remain true only if ALL patterns are ok
        for mu = 1:p      % check ALL patterns
            if sign(w * x(mu,:)') ~= 1
                w = w + eta * x(mu,:);
                converged = false;
            end
        end

        if converged
            break;
        end
    end

end

