function main()
maxIter = 100;
test = 0;
if test
    % 2 dimensional case for error debugging!
    n = 2;
    p = 100;
    % for testing purposed set this to 1
    linSep = 1;
    
    % show the 2 dimension case [testing]
    [x,t]= genData(n, p , linSep);
    [w,c] = pla(n,p,x,t, linSep, maxIter);
    plotSep(w,x,t,n);
end

% show the fraction of completion as function of p
n = 50;
p = [1:1:4*n];
ntrials = 10;
maxiter = 100;
eta = .1;
linSep = 0; % ensure the data is linearly separable?
% repeat experiment for ntrials
for triali = 1:ntrials
    for pi = 1:numel(p)
        [x,t] = genData(n,p(pi), linSep);
        [~,c] = pla(n, p(pi), x, t, eta, maxIter);
        cc(pi,triali) = c;
    end
end
cc = mean(cc,2);
figure(2);  clf()
plot(p./n, cc,'-o')
xlabel('P/N'); ylabel('Fraction of convergence');
ylim([0,1])


end

function [w,c] = pla(n, p, x, t, eta, maxiter)
% assuming that bias term is added
% init weights between -1, 1
w = rand(n+1,1)*2 - 1;

% convergence vector
c = zeros(p,1);
idx =1;
% keep going until all paterns are converged
while not(all(c))
    % for all patterns
    for j = 1:p
        inp = x(:,j);
        % compute the activation [binary neuron classication]
        tmp = sign(w' * inp);
        % if target is not achieved update weights
        if not(tmp == t(j))
            dw = eta * (t(j) - tmp) * inp;
            w = w + dw;
            c(j) = false;
            % converged
        elseif tmp == t(j)
            c(j) = true;
        end
    end
    
    % if converging is not possible stop
    if idx > maxiter
        %         fprintf('Not converged in maxiter\n')
        break
    end
    idx = idx +1;
end
% fprintf('Learning done \t Number of iterations %d \n', idx)
% number of converged patterns
c = 2^sum(c) /2^p;

end

function [x,t] = genData(n, p, sep)
% generates random linearly separable data, or random data
w = rand(n+1,1);
% generate data
for i = 1:p
    % add 1 for bias term
    tmp = [1; rand(n,1)*2-1];
    x(:, i) = tmp;
    % make sure they are linearly separable
    if sep
        t(i)  = sign(w' * tmp);
        
        % else just put random targets
        % note this may not lead to convergence!
    else
        if rand()*2 -1 > 0
            t(i) = 1;
        else
            t(i) = -1;
        end
    end
end
end

function plotSep(w,x,t,n)
if n == 2
    l = linspace(-1,1);
    % plot the line a x + b , with a = dxdy, b = dydz (due to bias)
    dxdy =  -w(2)/w(3);
    b    =  -w(1)/w(3);
    labels = {'1','-1','classification line'};
    figure(1); clf();hold
    
    % label 1 data
    tmpx = x(2,t == 1);
    tmpy = x(3, t == 1);
    % label -1 data
    tmpxx = x(2, t == -1);
    tmpyy = x(3, t == -1);
    
    % plot
    plot(tmpx, tmpy,'or',...
        tmpxx, tmpyy, 'ob',...
        l, dxdy * l + b, '--')
else
    disp('Too many dimensions! Try with 2');
    
end
end
%
% %% start of
%     case 'mnist',
%         load mnistAll.mat
%         Xmnist=mnist.train_images;
%         ymnist=mnist.train_labels;
%
%         X3=Xmnist(:,:,find(ymnist==3)); % images of 3's
%         X7=Xmnist(:,:,find(ymnist==7)); % images of 7's
%
%         n=size(X3,1).^2;
%         x3=-reshape(X3,n,size(X3,3));	% input data of 3's multiplied by class label -1
%         x7=reshape(X7,n,size(X7,3));		% input data of 7's multiplied by class label
%         x3(n+1,:)=-1;
%         x7(n+1,:)=1;
%         x=[x3,x7]';
%         x=double(x);	% matlab does not like the mnist data format
%         p=size(x,1);
%
% end;
% end
